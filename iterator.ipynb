{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8bc34a2b-43d1-4913-9e52-f82f51edc92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies and libraries #\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from datetime import date, timedelta\n",
    "import hvplot\n",
    "import alpaca_trade_api as tradeapi\n",
    "from finta import TA\n",
    "from pandas.tseries.offsets import DateOffset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "046064cc-2a6a-4f4c-be3d-6a25f98d9989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set Alpaca API key and secret by calling the os.getenv function and referencing the environment variable names\n",
    "# Set each environment variable to a notebook variable of the same name\n",
    "alpaca_api_key = \"PKT126TVACUEXXFT8Z85\"\n",
    "alpaca_secret_key = \"WwlEmn0WUNYY0iJL7ltRQypkzqNfojvcnFtScBIy\"\n",
    "\n",
    "# Check the values were imported correctly by evaluating the type of each\n",
    "display(type(alpaca_api_key))\n",
    "display(type(alpaca_secret_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ad10183-2ec1-44e6-b75e-48e6bd0d762d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your Alpaca API REST object by calling Alpaca's tradeapi.REST` function\n",
    "# Set the parameters to your alpaca_api_key, alpaca_secret_key and api_version=\"v2\" \n",
    "alpaca = tradeapi.REST(\n",
    "    alpaca_api_key,\n",
    "    alpaca_secret_key,\n",
    "    api_version=\"v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e39b0445-eed7-45f9-b89b-70b3c54ac8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker = \"TSLA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a6e35ea-e602-458d-8c0b-a71c00f9bcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dates\n",
    "\n",
    "start_date = pd.Timestamp(\"2019-10-01\", tz=\"America/New_York\").isoformat()\n",
    "end_date = pd.Timestamp(\"2022-10-19\", tz=\"America/New_York\").isoformat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "774340df-a6f2-44fd-9b53-43bbb225d674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timeframe \n",
    "timeframe = \"1Day\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd06bce2-6b4f-4791-a438-248b6906e3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Price information #\n",
    "df = alpaca.get_bars(\n",
    "    ticker,\n",
    "    timeframe,\n",
    "    start = start_date,\n",
    "    end = end_date,\n",
    ").df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b6c8db49-eff9-4746-b70f-17f3135424f6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create OHLCV df; drop 'trade_count' and 'vwap'columns\n",
    "df = df.drop(['trade_count', 'vwap'], axis=1)\n",
    "\n",
    "# Use the pct_change function to generate the returns from \"close\"\n",
    "df[\"actual_returns\"] = df[\"close\"].pct_change()\n",
    "\n",
    "# Drop all NaN values from the DataFrame\n",
    "df = df.dropna()\n",
    "\n",
    "# Generate the Input Features, X\n",
    "# Create additional technical indicators\n",
    "df['sma_slow'] = TA.SMA(df, 100)\n",
    "df['sma_fast'] = TA.SMA(df, 4)\n",
    "df[\"ssma\"] = TA.SSMA(df)\n",
    "df[\"ema\"] = TA.EMA(df, 50)\n",
    "df[\"dema\"] = TA.DEMA(df)\n",
    "df[\"tema\"] = TA.TEMA(df)\n",
    "df[\"trima\"] = TA.TRIMA(df)\n",
    "df[\"trix\"] = TA.TRIX(df)\n",
    "df[\"vama\"] = TA.VAMA(df)\n",
    "df[\"kama\"] = TA.KAMA(df)\n",
    "df[\"zlema\"] = TA.ZLEMA(df)\n",
    "df[\"wma\"] = TA.WMA(df)\n",
    "\n",
    "df = df.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "32beb7df-2ec0-4f03-8faf-901f5ff3f39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign a copy of the technical variable columns to a new DataFrame called `X` and shift values.\n",
    "# The shifted 'X' values will align a prior day's X values with the next day's 'y'/returns/trading signal,\n",
    "# to train the predictive model. \n",
    "X = df[['sma_slow', 'sma_fast', 'ssma', 'ema', 'dema', 'tema', 'trima', 'trix', 'vama', 'kama', 'zlema', 'wma']].shift().dropna().copy()\n",
    "\n",
    "# Initialize a `signal` column\n",
    "df['signal'] = 0.0\n",
    "\n",
    "# signal values will be based on the daily returns: positive returns yield '1', negative returns '-1'\n",
    "df.loc[(df['actual_returns'] >= 0), 'signal'] = 1\n",
    "df.loc[(df['actual_returns'] < 0), 'signal'] = -1\n",
    "\n",
    "# Copy the 'signal' column to a new Series called `y`.\n",
    "y = df['signal']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "973a5b85-5b57-4e26-b9d4-b5badbe8d82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_returns = {}\n",
    "for i in range(1,100):\n",
    "    \n",
    "    training_begin = X.index.min()\n",
    "    training_end = X.index.min() + DateOffset(weeks=i)\n",
    "    \n",
    "    # Slice the 'X' dataframe and 'y' Series into congruous training datasets.\n",
    "    X_train = X.loc[training_begin:training_end]\n",
    "    y_train = y.loc[training_begin:training_end]\n",
    "\n",
    "# Slice the testing 'X' and 'y' datasets, starting from the end of the training data until the most recent index. \n",
    "    X_test = X.loc[training_end:]\n",
    "    y_test = y.loc[training_end:]\n",
    "\n",
    "# Use StandardScaler to scale the X_train and X_test data.\n",
    "    scaler = StandardScaler()\n",
    "    X_scaler = scaler.fit(X_train)\n",
    "    X_train_scaled = X_scaler.transform(X_train)\n",
    "    X_test_scaled = X_scaler.transform(X_test)\n",
    "    \n",
    "    lrm=LogisticRegression()\n",
    "    lrm.fit(X_train_scaled, y_train)\n",
    "    testing_predictions = lrm.predict(X_test_scaled)\n",
    "    \n",
    "    lr_predictions_df = pd.DataFrame(index=X_test.index)\n",
    "    lr_predictions_df['predicted_returns'] = testing_predictions\n",
    "\n",
    "# Add in actual returns and calculate trading returns\n",
    "    lr_predictions_df['actual_returns'] = df['actual_returns']\n",
    "    lr_predictions_df['trading_algorithm_returns'] = lr_predictions_df['actual_returns'] * lr_predictions_df['predicted_returns']\n",
    "    lr_predictions_df['actual_cumulative'] = (1 + lr_predictions_df['actual_returns']).cumprod()\n",
    "    lr_predictions_df['algo_cumulative'] = (1 + lr_predictions_df['trading_algorithm_returns']).cumprod()\n",
    "\n",
    "    final_result_actual = lr_predictions_df['actual_cumulative'][df.index[-1]]\n",
    "    final_result_algo = lr_predictions_df['algo_cumulative'][df.index[-1]]\n",
    "        \n",
    "        \n",
    "    if final_result_algo > final_result_actual:\n",
    "        cum_returns[i] = final_result_algo\n",
    "\n",
    "max_value = max(cum_returns, key=cum_returns.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cc30f17c-ea9f-4cc9-8728-8d9ae55b4a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "training_begin = X.index.min()\n",
    "training_end = X.index.min() + DateOffset(weeks=max_value)\n",
    "    \n",
    "\n",
    "X_train = X.loc[training_begin:training_end]\n",
    "y_train = y.loc[training_begin:training_end]\n",
    "\n",
    "\n",
    "X_test = X.loc[training_end:]\n",
    "y_test = y.loc[training_end:]\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "    \n",
    "lrm=LogisticRegression()\n",
    "lrm.fit(X_train_scaled, y_train)\n",
    "testing_predictions = lrm.predict(X_test_scaled)\n",
    "\n",
    "print(testing_predictions[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b481afad-35b1-410e-a0ef-e3e33437d77e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26a82f3-8efb-4ee9-82b4-5d00e757c857",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
